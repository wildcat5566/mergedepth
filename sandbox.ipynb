{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import time\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from src.utils import *\n",
    "from src.loss import *\n",
    "from src.model import *\n",
    "from src.dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kitti_ds = KittiStereoLidar(\n",
    "    im_left_dir=glob.glob(\"data/left_imgs/*/*\"), \n",
    "    im_right_dir=glob.glob(\"data/right_imgs/*/*\"),\n",
    "    gt_left_dir=glob.glob(\"data/left_gt/*/*\"), \n",
    "    gt_right_dir=glob.glob(\"data/right_gt/*/*\"),\n",
    "    transform=transforms.Compose([transforms.Resize((197,645)),\n",
    "                                  transforms.ToTensor()])\n",
    ")\n",
    "\n",
    "# 2011_09_26_drive_0001_sync/\n",
    "# Resize issues\n",
    "# Original(375,1242)\n",
    "# (389,1285)->(384, 1280)->(160,320)\n",
    "# (197,645)->(192,640)->(96,320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21972\n"
     ]
    }
   ],
   "source": [
    "print(len(kitti_ds))\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(dataset=kitti_ds, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True, \n",
    "                          num_workers=6)\n",
    " \n",
    "dataiter = iter(train_loader) #21972"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depth prediction networks for left & right view sets respectively\n",
    "L = Network()\n",
    "L = torch.nn.DataParallel(L).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precalculate mapping parameters\n",
    "sc = 320/1242\n",
    "reconstruct_functions = [Reconstruction(date='2011_09_26',scaling=sc), \n",
    "                         Reconstruction(date='2011_09_28',scaling=sc),\n",
    "                         Reconstruction(date='2011_09_29',scaling=sc), \n",
    "                         Reconstruction(date='2011_09_30',scaling=sc),\n",
    "                         Reconstruction(date='2011_10_03',scaling=sc)]\n",
    "\n",
    "def normalize_prediction(map_input, scale=100):\n",
    "    M, m=np.amax(map_input), np.amin(map_input)\n",
    "    return (map_input - m)*(scale / (M-m))\n",
    "\n",
    "def get_unsu_loss(depth_maps, src_imgs, tar_imgs, direction, dates):\n",
    "    \n",
    "    batch_loss = 0\n",
    "    for[dep, src, tar, dat] in zip(depth_maps, src_imgs, tar_imgs, dates):\n",
    "        if dat=='2011_09_26':\n",
    "            recf = reconstruct_functions[0]\n",
    "        elif dat=='2011_09_28':\n",
    "            recf = reconstruct_functions[1]\n",
    "        elif dat=='2011_09_29':\n",
    "            recf = reconstruct_functions[2]\n",
    "        elif dat=='2011_09_30':\n",
    "            recf = reconstruct_functions[3]\n",
    "        elif dat=='2011_10_03':\n",
    "            recf = reconstruct_functions[4]\n",
    "\n",
    "        # Calculate sample loss\n",
    "        sample_loss, img = recf.compute_loss(dep, src, tar, direction)\n",
    "        batch_loss += sample_loss\n",
    "\n",
    "    return batch_loss / batch_size, img\n",
    "\n",
    "def get_su_loss(depth_maps, scan_files):\n",
    "    \n",
    "    batch_loss = 0\n",
    "    for[dep, scan_file] in zip(depth_maps, scan_files):\n",
    "        dots = np.load(scan_file) \n",
    "\n",
    "        sample_loss = gt_loss(dep, dots)\n",
    "        batch_loss += sample_loss\n",
    "\n",
    "    return batch_loss / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "alpha = 1\n",
    "beta = 1\n",
    "lr = 5e-2\n",
    "L_optimizer = torch.optim.SGD(L.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tStep Loss: 0.052659 \t Su/Unsu: (tensor(1.00000e-02 *\n",
      "       3.6463, device='cuda:0'), tensor(0.1747, device='cuda:0'))\n",
      "Epoch: 1 \tStep Loss: 0.048855 \t Su/Unsu: (tensor(1.00000e-02 *\n",
      "       3.6765, device='cuda:0'), tensor(0.1565, device='cuda:0'))\n",
      "Epoch: 1 \tStep Loss: 0.046527 \t Su/Unsu: (tensor(1.00000e-02 *\n",
      "       3.0995, device='cuda:0'), tensor(0.1222, device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "L.train()\n",
    "img=None\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    batch_count = 1\n",
    "    print_every = 10 #50\n",
    "    time_start = time.time()\n",
    "\n",
    "    for images_l, images_r, scans_l, scans_r in train_loader: #images_l: torch.Size([batch_size, 3, 197, 645])\n",
    "        L_optimizer.zero_grad()\n",
    "        \n",
    "        # Move to cuda\n",
    "        images_l = images_l.cuda()\n",
    "        images_r = images_r.cuda()\n",
    "        \n",
    "        # Forward pass, make predictions\n",
    "        depths_l = L(images_l)\n",
    "\n",
    "        # Back to numpy\n",
    "        drive_dates = [s[13:23] for s in scans_l]\n",
    "        \n",
    "        # Compute losses\n",
    "        su_loss_L = get_su_loss(depth_maps=depths_l, scan_files=scans_l)\n",
    "        unsu_loss_L2R, img = get_unsu_loss(depth_maps=depths_l, \n",
    "                                      src_imgs=images_l, \n",
    "                                      tar_imgs=images_r, \n",
    "                                      direction='L2R', dates=drive_dates)\n",
    "        \n",
    "        loss = alpha*su_loss_L + beta*unsu_loss_L2R\n",
    "\n",
    "        # Back propagation & optimize\n",
    "        loss.backward()\n",
    "        L_optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        step_loss = train_loss / (batch_count * batch_size)\n",
    "        if batch_count % print_every == 0:\n",
    "            print('Epoch: {} \\tStep Loss: {:.6f} \\t Su/Unsu: {}'.format(\n",
    "                epoch+1, \n",
    "                step_loss,\n",
    "                (su_loss_L, unsu_loss_L2R)\n",
    "            ))\n",
    "        batch_count += 1\n",
    "\n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss / len(train_loader.sampler) #image pair count\n",
    "    time_elapsed = time.time() - time_start\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tTime: {} s'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        round(time_elapsed, 4)\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img.shape\n",
    "#r = np.transpose(img.cpu().detach().numpy(), (1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(r)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
